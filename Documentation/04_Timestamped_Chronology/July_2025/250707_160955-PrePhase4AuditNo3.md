System Audit: TOASTer, JDAT, JMID, and JVID FrameworksTOASTer Application (Transport-Oriented Audio Sync Tunnel)Implemented Functionality: The TOASTer app (formerly called MIDILink) is a GUI application built to manage network transport and sync for the JAMNet system. It has been successfully renamed and refactored from the old MIDILink appgithub.com. All core TCP-based transport features are implemented and functional – the app can act as a TCP server or client, send/receive JSON-formatted MIDI messages, and maintain connections. In recent updates, the developers confirmed “All TCP TOAST components [are] built and functional,” ready for multi-client LAN testinggithub.com. Basic MIDI message exchange works in real time between a TOASTer client and server, and the UI includes multiple panels (for clock sync, network connection, performance monitoring, etc.) to facilitate testing these features. The app builds and runs, and internal tests show that a client can send MIDI data to a server, which processes it and even sends acknowledgments backgithub.com. This confirms the TCP transport layer for MIDI is operational with thread-managed I/O and ~microsecond-level processing latencygithub.com. Broken or Incomplete Areas: Despite the progress on the TCP side, UDP-based transport sync is not yet implemented. The current TOASTer version does not actually use UDP for transport synchronization – all networking still runs over TCP sockets. The plan (as stated in the project logs) was to begin adding UDP support after establishing the TCP foundationgithub.com. At this time, transport sync over UDP is essentially placeholder-only: there is no active UDP packet exchange for sync in the codebase. The app’s next development step was explicitly “UDP + PNTBTR development” (predictive recovery for packet loss)github.com, implying these features remain to be done. Therefore, transport synchronization currently relies on TCP; things like tempo or timeline sync are handled by the reliable TCP channel, which could introduce latency. This is a known limitation to be addressed. Additionally, some UI components in TOASTer appear to be in flux – for example, there are “MIDITestingPanel_new” vs “_old” source files, suggesting a refactoring in progressgithub.com. Such duplicate panels indicate parts of the GUI or testing logic are not finalized or might be broken pending completion of the new implementation. Finally, the TOASTer app currently focuses on MIDI networking; there is no integrated UI or controls for audio (JDAT) or video (JVID) streaming yet, which will be needed in Phase 4. UDP Transport Sync Handling: At present, TOASTer does not actively utilize UDP for sync. The design calls for using UDP broadcasts or datagrams to achieve sub-millisecond timing sync across nodes, but this is not in place yet. All synchronization (e.g. clock signals, start/stop commands) is going through the TCP connection for now. This means the system likely suffers higher latency than desired for transport control. The codebase includes plans for a UDP-based “TOAST” protocol mode – referenced in documentation and pending features – but as of the latest commit, UDP support is unimplemented. In summary, transport sync over UDP is **currently not handled (no code) and remains a to-do item for the next phase.JDAT Framework (JSON Audio Transport / “JSONADAT”)Current State: The JDAT framework is intended for professional, low-latency audio streaming in the JAMNet ecosystem. In the repository it appears under the name JSONADAT_Framework, reflecting a JSON-based audio protocol inspired by ADAT. The latest integration commit indicates that the JDAT framework has been sketched out with core components: “JSONADAT_Framework: Ultra-low latency audio streaming with JELLIE encoding – complete header files, schemas, and example implementations”github.com. Indeed, the source tree contains stubs for audio encoding/decoding (e.g. JELLIEEncoder.h, JELLIEDecoder.h), buffer management, a lock-free queue for audio packets, and even example demo programs (like adat_192k_demo.cpp)github.comgithub.com. However, this implementation is largely a placeholder at this stage. The presence of header definitions and JSON schema files (e.g. jsonadat-message.schema.json) shows the protocol format is defined, but there’s no evidence that JDAT has been fully realized or tested in operation. No commit logs mention successfully streaming live audio yet, and the integration was done as part of a bulk update rather than incremental testing. Not Yet Functional: It appears the JDAT protocol is not actually functional yet in the running system. The framework is present but likely incomplete:No integration with TOASTer – The TOASTer app does not yet use JDAT; there are no UI panels or logic in the app for capturing or playing audio via the network.Unproven encoding – The custom “JELLIE” encoding for audio is described, but it’s unclear if this is implemented beyond skeleton code. The code might simulate audio frames in the examples, but real audio device I/O or compression may be missing.Protocol as a Placeholder – Essentially, JDAT exists as a defined schema and class set, but functions may be stubbed or unoptimized. For instance, classes like WaveformPredictor and ADATSimulator hint at future features (predictive audio reconstruction, simulation), yet those are likely not fully fleshed out.No network test – Unlike JMID, which has a toast_client/server test, JDAT has no demonstrable client/server implementation in use. This suggests the audio streaming hasn’t been executed in practice yet.In summary, JDAT is currently a placeholder/prototype. The groundwork is laid (data structures, JSON message format, some algorithmic scaffolding), but the protocol is not operational in a live setting. To support Phase 4, this framework will need significant development: implementing the actual audio capture/playback pipeline, optimizing the JELLIE codec, and integrating JDAT streams into the TOASTer network transport (likely over UDP for latency). Until those are done, JDAT remains incomplete.JMID Framework (JSON MIDI Transport)Current State: The JMID framework (found in the code as JSONMIDI_Framework) is the most mature of the three, focusing on MIDI message streaming. This was the original core of the project (MIDIp2p), and as such it is partially to fully implemented. Recent commits confirm that the basic MIDI-over-network functionality works reliably. In Phase 2.2, the team achieved a major milestone: “TCP communication working – client sends MIDI messages to server, server receives and processes all messages, server sends acknowledgment responses back… Performance: <1μs message processing”github.comgithub.com. They have a dedicated transport class (TOASTTransport) handling JSON serialization/deserialization and networking, and test programs (toast_server.cpp and toast_client.cpp) were created to validate MIDI exchange over TCPgithub.comgithub.com. In effect, JMID’s core networking layer is functional: multiple clients can connect to a TOASTer server and MIDI events (probably encoded as JSON objects) are transmitted in real time. The system uses separate threads for I/O and even includes clock drift handling mechanisms (a “ClockSyncModule” is present) to keep MIDI timing alignedgithub.com. Working vs. Missing Features: While the basic MIDI streaming works, JMID is not feature-complete. What’s working now is point-to-point or LAN MIDI data exchange over TCP, proven with two-process testsgithub.com. But for full Phase 4 needs:UDP transport not done: The plan is to support UDP multicast or broadcast for MIDI (to reduce latency and enable group jam sessions), but currently all MIDI data rides on TCP. The project explicitly notes that the foundation is ready for “UDP + PNTBTR” nextgithub.com, meaning JMID hasn’t transitioned to UDP yet. Until UDP is added, MIDI jitter might be higher than ideal.Clock sync / tempo: The presence of clock sync code suggests an attempt at keeping MIDI timing consistent, but it’s unclear if tempo map or MIDI clock messages are actually implemented or just planned. This may require further refinement and testing across hosts.Error handling and recovery: The protocol uses JSON, which is heavier than raw MIDI. The commit history references predictive recovery (PNTBTR) for lost packetsgithub.com, but it’s not clear if MIDI stream has any retry or loss handling implemented yet. Likely, these advanced features are still not realized.MIDI 2.0 or advanced messages: There’s no mention if the system supports only basic MIDI 1.0 messages or the newer MIDI 2.0/UMP format. Given the early stage, support is probably limited to standard MIDI messages encapsulated in JSON.Despite these gaps, JMID can be considered the only framework currently functional in the JAMNet suite. It provides a working baseline for MIDI data exchange. For Phase 4, the main improvements needed are to implement the UDP transport mode, ensure robust synchronization (perhaps using high-resolution clocks and drift correction across peers), and integrate JMID seamlessly with the upcoming audio/video so that all media types stay in sync.JVID Framework (JSON Video/Vector Transport)Current State: The JVID framework (in code as JSONVID_Framework) targets real-time visual streaming (likely to send vector graphics or video frames, possibly from a “JAMCam” as mentioned). As of now, JVID appears to be at a conceptual or skeleton stage. The recent repository update added a JSONVID framework directory with core placeholders: there are header files for a JAMCamEncoder/Decoder, a FramePredictor, VideoBufferManager, etc., plus a jsonvid-message.schema.json defining the JSON message formatgithub.comgithub.com. An example basic_jamcam_demo.cpp is provided, presumably as a dummy illustration of usagegithub.com. This mirrors how the audio framework was added – structure without full substance. There is no evidence of actual video encoding or network streaming logic yet beyond these definitions. Missing/Incomplete: Practically, JVID’s implementation is missing:No real video processing – Classes like JAMCamEncoder likely outline how one would take video frames or vector graphics and pack them into JSON, but these are not known to be implemented. The lack of any mention of successful video streaming in commit messages confirms this. The components are likely empty methods or pseudocode awaiting actual encoding algorithms.Untested performance – Video streaming is bandwidth-intensive, and using JSON adds overhead. There’s no indication that a working prototype has sent video frames over the network. The targeted latency (~300μs per the README) is purely theoretical at this point.Not integrated – TOASTer app does not reference JVID at all. No UI or controls for video streaming exist yet, meaning JVID isn’t hooked into any runnable application. It’s effectively dormant code.Vector vs. raster – It’s unclear if “vector or visual streaming” (as the question phrased) is meant for vector graphics or actual video. The naming “JAMCam” implies camera video frames. Regardless, no concrete implementation for either is present, aside from some planning docs.In summary, JVID is presently just a framework outline (placeholder). It has a defined JSON schema and class scaffolding but no functional streaming. To reach Phase 4 goals, this framework will need to be fully built out: encoding of video or graphics data into JSON packets, efficient transport (likely needing UDP multicast), and synchronization with audio/MIDI. Until then, JVID remains an empty shell with no active role in the system.Overall Interoperability and Phase 4 ReadinessAcross these components, the JAMNet system has a unified vision but uneven implementation. The design philosophy is that all media types (MIDI, audio, video) share a common transport protocol (TOAST) using JSON messages, enabling human-readable real-time collaborationgithub.comgithub.com. In practice, as of now only the MIDI part is truly working, while audio and video are mostly aspirational. This creates several technical gaps and inconsistencies that must be addressed before Phase 4:Incomplete UDP Support: Ultra-low latency sync is a core requirement (the project touts “microsecond latencies” for each stream). However, the current system still relies on TCP, which, while functional, is not ideal for time-sensitive media. The plan for UDP must be executed. This includes implementing the PNTBTR (Predictive Network Time-Base Recovery) system for packet loss recoverygithub.com. At the moment, PNTBTR is only mentioned on paper; deploying it will be crucial for stable audio/video streaming over UDP in Phase 4.Integration of Audio/Video Streams: TOASTer and the JSONMIDI transport need to integrate JDAT and JVID so that all streams can operate together. Right now, TOASTer is essentially a “MIDI over network” app. For Phase 4, it must evolve to handle audio and video concurrently. That means adding audio input/output handling in the app, mixing or routing, and possibly a UI for video feeds or shared visuals. The frameworks for JDAT/JVID will need to move from placeholders to fully tested modules that can plug into TOASTer.Consistency and Code Cleanup: There are some remnants of older naming and possibly duplicate code paths (for example, both JDAT_Framework and JSONADAT_Framework directories exist in the repo). Such inconsistencies suggest the project underwent renaming/refactoring and may not have removed all deprecated pieces. Before Phase 4, the codebase should be cleaned up and unified – only one set of framework code for each function, consistent naming (the team has largely done this by renaming MIDILink to TOASTer, etc. github.com), and ensuring documentation matches the code. Any design docs or schemas must align with implemented behavior.Performance and Testing: Thus far, only basic LAN loopback tests for MIDI have been done. Significant testing is required for the whole system under real-world conditions. This includes measuring latency for audio and video once implemented, testing synchronization of MIDI–audio–video (e.g., does a MIDI metronome align with audio beats and video cues across distributed clients?), and ensuring the JSON approach is viable at scale. The choice of JSON for high-throughput streaming is unconventional; Phase 4 should validate that custom optimizations (like the lock-free queues and predictive algorithms mentionedgithub.com) actually compensate for JSON overhead. If any performance bottlenecks or jitter issues arise, design adjustments may be needed (potentially considering binary packing inside JSON or hybrid approaches).Robustness: Features like error handling, reconnection logic, and session management are not fully detailed in the current state. The MIDI transport has basic connect/disconnect handlinggithub.com, but when expanding to audio/video, the system must handle dropped packets (hence PNTBTR), network jitter, and maybe dynamic peer discovery (the presence of a Bonjour service test file suggests auto-discovery was being explored). These aspects are not yet proven and will need solid implementation for a production-ready Phase 4.In conclusion, the JAMNet/MIDIp2p project is partway to its goal. The MIDI (JMID) portion is operational over TCP, but audio (JDAT) and video (JVID) are mostly skeletal. The TOASTer app provides a foundation for user interaction and sync, but it currently covers only MIDI and TCP-based transport. Phase 4 development should focus on completing the JDAT and JVID implementations (encoding, decoding, real streaming), enabling UDP transport and sync for all media, and rigorously testing the entire system’s interoperability. Additionally, any architectural inconsistencies should be resolved now – ensuring a cohesive design where all three frameworks truly work in tandem via the TOASTer protocol. With these improvements, the project will be better prepared to achieve full multimedia peer-to-peer jamming with the promised ultra-low latency
