Understood. I’ll now analyze the latest version of the MIDIp2p GitHub repo, focusing specifically on the current state of TOAST and TOASTer, and begin drafting a detailed CPU-to-GPU clock sync API for Phase 4. I’ll return shortly with a full plan covering the sync strategy, API schema, and DAW-facing bridge module to ensure seamless future integration with legacy CPU-bound DAWs.
Phase 4 CPU–GPU Sync API Specification
Current State of TOAST and TOASTer (GPU-Native Transport)
TOAST Protocol: Transport Oriented Audio Synchronization Tunnel (TOAST) is the core real-time transport protocol in the JAMNet ecosystem. It runs over UDP multicast and provides a unified clock synchronization mechanism to achieve microsecond-level latency for audio/MIDI streaming. TOAST defines a custom frame structure with various packet types (MIDI, audio, video, sync, transport, etc.) and includes a 32-byte header containing timing and sequence information. Crucially, TOAST uses a GPU-driven master timebase: the GPU provides a high-resolution clock that acts as the session’s timing reference, rather than relying on CPU timing. This enables GPU-clocked send/receive threads and a “GPU Master Timeline” that keep streams in lockstep with sub-millisecond precision. In practice, TOAST achieves “unified clock sync” across all peers by timestamping frames with the GPU’s clock and synchronizing playback based on that timeline.
TOASTer App: The TOASTer application is an open-source reference implementation and testbed for the TOAST protocol. It provides a GUI (built with JUCE) to manage network sessions, visualize performance, and control transport. TOASTer can create or join a multicast session and automatically discovers peers via Bonjour/mDNS. Once connected, it enables bidirectional transport control – i.e. any instance can play, stop, or seek and the others follow. Currently, play/pause synchronization is implemented by broadcasting JSON-formatted transport commands (“PLAY”, “STOP”, etc.) to all peers. Each command includes the session position timestamp and BPM, allowing receivers to align their timelines. The TOASTer UI includes transport buttons (play/stop/record) and a session time display that updates in real time, showing hours:minutes:seconds.microseconds and musical bars/beats. When the user hits play, TOASTer captures the current high-resolution time and current song position, then calls sendTransportCommand("PLAY", timestamp, position, bpm) on the JAMNet framework. This is sent as a JSON message over the network. On the receiving side, TOASTer’s handler will update its state only if it is not the designated master, toggling play and rebasing its local clock to align with the incoming timestamp and position. For example, when a “PLAY” command is received, the slave instance computes a new transportStartTime such that its local clock plus offset matches the master’s timeline. This ensures all peers start in sync despite network delay. Basic tempo and seek messages are also defined (e.g. “BPM” or “POSITION” commands), though the current UI doesn’t expose a full timeline scrubber – these commands would be used by an external DAW integration.
Latency & Drift Management: TOASTer and the JAMNet core have initial provisions for handling clock drift and network latency, though this is an evolving area. In the code, a ClockDriftArbiter object is responsible for continuous clock alignment between peers. The TOASTer UI includes a Clock Sync Panel with an “Enable Sync” toggle and a calibration button, indicating that the system can perform periodic sync refinements. When enabled, the ClockDriftArbiter will elect a master clock (or honor a forced master) and compute the offset and round-trip latency to each peer. The panel displays the device’s role (Master or Slave), the measured network offset between clocks, a “sync quality” metric, and round-trip time (RTT). Internally, the arbiter likely exchanges timestamped sync messages (similar to a PTP/NTP protocol) to measure one-way latency and clock skew. For example, the UI updates show that for a connected peer, it retrieves realLatency = getNetworkLatency(peer) and realDrift = getClockDrift(peer), then updates the offset and quality accordingly. Currently these values are placeholders or simplistic (quality is computed as max(0, 1 – latency/100) in the demo), but the framework is in place to refine them. In summary, TOAST’s current state provides the low-level means to sync clocks and send transport events over the GPU-accelerated network, but the integration with external systems (like traditional DAWs) is minimal so far – this is where Phase 4 comes in.
Phase 4 Sync API Design Overview
Objective: Phase 4 focuses on interoperability between JAMNet’s GPU-timed network and conventional CPU-bound DAWs. The goal is to design a CPU–GPU sync API and bridging layer that allows a DAW (or a plugin within a DAW) to lock its transport and audio/MIDI streams to the JAMNet session. This involves translating between the GPU master clock domain used by JAM/TOAST and the DAW’s own timing (typically driven by the audio interface clock or system clock), achieving seamless play/pause, tempo, and position sync in both directions. The guiding principles are: keep the GPU as the high-precision time source whenever possible (to leverage its stability), introduce minimal CPU overhead in the signal path, and ensure legacy DAWs can participate without modifications to their core timing engines (the integration will happen via standard plugin or sync interfaces). The API will essentially act as a bridge between the GPU-native JAMNet world and the DAW environment, handling clock conversion, transport messaging, and buffered audio/MIDI data exchange.
API Schema and Data Structures
Transport Command Format: We define a structured message format for all transport and tempo sync commands, using a simple JSON schema (consistent with JAMNet’s JSONL philosophy). A transport command message includes: the command type, a high-precision timestamp, the current song position, and tempo. For example:
{
  "type": "transport",
  "command": "PLAY",         // or "STOP", "POSITION", "BPM"
  "position": 120000.0,      // musical position in milliseconds (or microseconds) 
  "bpm": 128.0,              // tempo at time of command
  "timestamp": 1699212345678900   // GPU nanosecond timestamp when issued
}
This format largely mirrors what the current JAMNet implementation sends inside TOAST frames. It ensures that whenever a play/pause or seek occurs, all relevant info is broadcast: e.g. on PLAY, the current BPM and position (in the song) accompany the command. The timestamp field is the key to cross-domain sync: it captures the absolute GPU clock time when the command was initiated. The receiving side (the DAW or plugin) will compare this to its own clock to calculate latency and adjust timing (explained below). POSITION messages (for explicit seeks) and BPM messages (for tempo changes) use the same schema, with command set accordingly and the new position or bpm value included. These can be sent standalone (e.g. if a user drags the playhead in the DAW, a POSITION message goes out) or in combination with a PLAY (if a seek+play happens). The STOP command may include a position as well (indicating where playback stopped), though that’s optional. By using JSON, this schema can be transported either over sockets (to a plugin or external app) or embedded in TOAST frames for network broadcast – it is human-readable and extensible.
Clock Sync Messages: In addition to transport commands, the API will support low-level clock sync messages to continuously reconcile GPU and CPU clocks. These are not directly seen by end-users but are used under the hood by the ClockDriftArbiter. The protocol resembles PTP (Precision Time Protocol): for example, the DAW plugin can regularly send a Ping message containing [ "type": "sync", "sendTime": <GPU_time> ] to the JAMNet engine, which records the arrival with its GPU clock. The engine might respond with a Pong containing its current time and echoing the received timestamp (or vice-versa, with JAMNet initiating). Using these exchanges, both sides can compute round-trip delay and offset. Internally, the API’s data structure for this might be a small binary packet or JSON like {"type":"sync","t1":<gpu_send>,"t2":<cpu_recv>,"t3":<cpu_send>,"t4":<gpu_recv>}, carrying the four timestamps needed to solve for clock offset and network delay. This is analogous to how NTP sync works. The ClockDriftArbiter in TOASTer will encapsulate this logic – the API just needs to expose an interface to retrieve the calculated offset/drift and update the conversion formula used for timebase translation. In summary, the schema consists of: (a) Transport messages as above, and (b) Sync messages for ongoing clock calibration (exact format can be JSON or compact binary since they occur frequently). All messages can be carried over a chosen transport (shared memory, socket, etc.) which we discuss later.
Shared Data Buffer Structure: To exchange audio/MIDI streams between the GPU realm and the DAW, the API will provide shared buffer structures. For instance, a ring buffer (lock-free queue) in shared memory could hold outgoing audio packets from JAMNet for the DAW to consume. A corresponding structure might look like:
struct AudioBufferHeader {
    uint64_t gpuTimestamp;   // GPU time when this buffer was generated (ns)
    uint64_t audioPosition;  // Position in song timeline (e.g. sample index or microsec)
    uint32_t numFrames;
    uint32_t sampleRate;
    uint8_t  channels;
    // ... perhaps a sequence number or flags
};
float audioSamples[numFrames * channels];
The GPU/JAM side would write audio samples into such a buffer along with the timestamp and position. The DAW/plugin side reads buffers in order and knows exactly at what timeline position and time they correspond to. A similar structure would exist for MIDI events (which are sparse; these could be queued as timestamped MIDI messages or packaged in small blocks). By including both a high-res timestamp and a musical position indicator, the DAW can choose how to schedule the audio: either lock to wall-clock time or align to its bar/beat grid. (In practice, using the continuous timestamp is key for sub-millisecond accuracy; the musical position is more for aligning to bar boundaries after large seeks, etc.)
API Functions: If the DAW integration is provided as an SDK (for plugin developers or DAW vendors), the API would include function calls such as:
	•	initializeSync(sessionID, mode) – join a JAMNet session and set mode (e.g. host as master or slave).
	•	translateGpuTimeToCpu(uint64_t gpuTime) – convert a GPU nanosecond timestamp to the DAW/CPU clock time (e.g. host sample time or microseconds). And translateCpuTimeToGpu(uint64_t cpuTime) for reverse.
	•	sendTransportCommand(command, position, bpm) – push a PLAY/STOP/seek command into the network (the API will fill in the GPU timestamp).
	•	registerTransportCallback(fn(command, position, bpm)) – to be notified when a transport event is received from JAMNet (so the DAW can respond).
	•	pullAudioFrame(float* buffer, uint32_t frames, Info* info) – retrieve the next audio buffer from the JAMNet stream, filling into the DAW’s buffer; info would carry timing metadata like the frame’s timestamp or position.
	•	pushMidiEvent(MIDIEvent ev) – send a MIDI event from DAW to JAMNet (timestamped with DAW time, which the API will convert to the GPU timeline before sending).
These calls abstract away the underlying shared memory or socket – under the hood, they read/write to the shared buffers or send JSON messages as appropriate. The API will also expose status queries: e.g. getSyncStatus() returning current latency, jitter, drift rate, etc., and getPredictionConfidence() to fetch the latest PNBTR confidence (more on this below).
Synchronization Interval and Clock Drift Handling
Recommended Sync Interval: We propose that the GPU–CPU clocks be resynchronized at a steady interval – on the order of 10–50 ms (20–100 Hz). The TOASTer clock panel currently defaults to 24 Hz for sync updates, which is a reasonable starting point: ~42 ms intervals. This frequency is a trade-off between responsiveness to drift vs. network/CPU overhead. At 24 Hz, any drift or offset is corrected quickly (within a few dozen milliseconds) without flooding the system with sync packets. The API should allow adjusting this rate: for example, a slider from 1 Hz (very relaxed sync) up to ~100 Hz for extremely tight coupling in critical scenarios. In most cases 20–50 Hz will keep clock error below a millisecond, given typical crystal drift rates. The sync messages described earlier (Ping/Pong exchanges) will be sent at this interval. Each message is small (a few timestamps), so even at 100 Hz the bandwidth impact is negligible. The heartbeat messages already in TOAST could double as sync beacons – indeed TOAST currently sends a heartbeat every second, but for DAW sync we need a much faster tick than 1 Hz. We will repurpose or augment that mechanism. The ClockDriftArbiter can orchestrate these periodic sync polls.
Latency and Offset Compensation: On each sync exchange, the plugin/host measures one-way latency and clock offset between GPU and CPU. The Phase 4 API will maintain a continuously updated offset value Δt = t_gpu – t_cpu (or vice versa) and an estimate of relative drift (difference in clock rate). All time conversions will use these. For example, if a JAMNet audio packet arrives with a GPU timestamp of T_gpu, and the current offset says GPU clock is 150 μs ahead of the local CPU clock, the plugin knows that packet corresponds to CPU time T_cpu = T_gpu – 150µs. It can thus schedule playback so that the audio plays out exactly when it was supposed to in the session timeline. Similarly, when the DAW issues a transport command, it should include both its local timestamp and the corresponding GPU time. The API can stamp the message with GPU time if the GPU clock is accessible on the same machine (on a single machine, we might directly query the GPU high-res timer via an OpenCL, CUDA or Metal query). If not, it uses the last known offset to approximate the GPU time at send.
Clock Drift Re-basing: If a seek or abrupt transport change occurs, the sync API must re-base the clocks to the new timeline. For instance, consider a user scrubbing the DAW playhead to a new position (say 2:00 into the song) while paused. The plugin will send a {"command":"POSITION", "position":120000ms, "bpm": currentBpm} message out. All JAMNet peers, upon receiving this, will immediately update their timeline reference. On the GPU side, this could be handled by adjusting the position offset used in the session. One simple approach is: when a POSITION command is received, set currentPosition = position and, if playback is currently running, reset the internal start time reference so that the next audio frame aligns to the new position. Essentially the GPU timeline continues running (we don’t reset the GPU’s clock), but we change the mapping of that clock to “bars:beats”. The API will similarly inform the DAW to jump its playhead to the new location (if the seek came from another peer). Because both sides know the command timestamp, they can calculate where they should be at the moment of applying the seek. If the seek is done while playing (e.g. jump on the fly), a more careful approach is needed: ideally schedule the jump at a common future time. For Phase 4, a simpler method is to pause, reposition, then play – this guarantees alignment without needing sub-buffer scheduling. The spec will define that any “POSITION” command implicitly causes a pause in all systems, moves to the new spot, and then either waits for a separate “PLAY” or includes a flag to resume immediately. This avoids race conditions or mid-stream discontinuities. When resuming after a seek, a fresh round of clock sync can be done to ensure no residual offset.
Ongoing Drift Correction: Even with clocks initially synced, two independent clocks will drift over time. The API will implement a phase-lock loop (PLL) to constantly nudge the DAW’s audio timing toward the GPU master clock. On each sync interval, the measured offset Δt is fed into a filter. If the offset stays near zero, all is well; if a consistent drift is observed (offset increasing or decreasing steadily), the DAW plugin can adjust playback speed slightly. There are a few ways to do this in a DAW context:
	•	If the DAW supports fractional sample rates or tempo adjustment, the plugin could alter the tempo by a tiny amount to realign. However, most DAWs don’t allow plugins to directly change global tempo or sample rate on the fly.
	•	A more practical approach: buffer scheduling. The plugin can time the delivery of audio buffers based on the GPU clock rather than blindly on the DAW’s nominal buffer rate. For example, if the DAW is pulling audio in 128-sample blocks, that’s ~2.9 ms at 44.1 kHz. The plugin knows when each block should be played according to the JAMNet timeline (by converting the JAM timestamp to the DAW clock). If it detects it’s consistently finishing buffers a bit too early (meaning the DAW is running faster than JAM’s clock), it can intentionally pad or delay a few samples (e.g. deliver 129 samples occasionally, or insert tiny silence) to slow down. Conversely, if the DAW is lagging, the plugin can drop or shorten a buffer slightly to catch up. These adjustments would be extremely small (microseconds) and spread over time, so as to be inaudible. Essentially, the plugin acts as a resampling buffer, using the drift info to very lightly time-stretch the stream and keep the clocks phase-aligned. The high frequency of sync (20–50 Hz) means the adjustments can be very subtle each time.
	•	Meanwhile, the JAMNet side could also accommodate minor drift: since GPU is master, ideally the DAW follows it. But if the DAW’s clock cannot be altered, the JAM side could decide to adapt slightly. In a hybrid mode (discussed below), both sides share responsibility: e.g. JAMNet might allow its next packet deadline to slip a tiny bit if it sees the DAW consistently behind, rather than the DAW doing all the stretching.
The API will expose the measured drift and any applied compensation so that host systems are aware. In critical scenarios, if drift grows beyond a threshold (say one system’s clock is significantly off), the API could issue a warning or even perform a clock re-sync calibration (like re-running the master election or offset measurement at high precision). The presence of PNBTR (predictive buffering) also helps here: if a slight mismatch causes a sample or two of under-run, PNBTR’s neural prediction can fill in the gap seamlessly. In effect, PNBTR provides a cushion allowing the system to time-stretch or compress by a tiny fraction without artifacts, increasing confidence that drift corrections won’t be audible.
Transport and Tempo Round-Trip Sync
Bidirectional Transport Control: The Phase 4 sync API enables round-trip transport sync, meaning either the DAW or the JAMNet side can initiate a play/stop/seek and the other will follow. To avoid conflict or loopback, the system will use a simple master arbitration: if the DAW user presses play, the DAW (plugin) sends a “PLAY” command out to JAMNet but does not re-broadcast any “PLAY” it receives (it knows this event originated locally). Conversely, if JAMNet (or a remote peer) initiates playback, the plugin receives a “PLAY” message and triggers the DAW transport, but will not echo that back to the network. This logic is already partly handled by the isMaster flag in TOASTer’s TransportController (the master doesn’t accept remote play commands). In a DAW context, the plugin can either automatically designate the DAW as master when the user explicitly uses the DAW controls, or follow an external master if configured so. The API will include a “force master” or “auto” mode setting: Force Master means the DAW’s transport always drives the network (suitable when the user is primarily using the DAW UI to control playback), whereas Auto/Election means the first to press Play becomes leader until stop, etc., or it could defer to JAMNet’s built-in election (for example, among multiple peers not in a DAW, one might be master). We anticipate that in many cases the user will want the DAW to be the timeline leader (since they might be recording or arranging there), but the design allows either side to lead.
Implementing DAW control from a plugin depends on DAW capabilities. In some hosts, a plugin can start/stop the transport via host API (e.g. VST3 has an interface for transport states). If direct control isn’t available, we may fall back to sending standard MIDI Machine Control or MIDI Clock signals from the plugin to trick the DAW into following – many DAWs can be slaved to MIDI clock or MTC (though resolution is coarse). For higher precision, we prefer native integration: e.g. as a Max for Live device (Ableton) or using ReWire/Ableton Link-like APIs if available. The spec will document integration guidelines for popular DAWs. The key point is that when a JAMNet transport command arrives, the DAW should engage its transport accordingly, and vice versa, with minimal lag. Given that TOAST and the plugin know the exact timestamp of the command, we can schedule it accurately. For example, if a PLAY comes from the network with timestamp T0 (GPU time) and the current offset says that corresponds to local time T0’, and we see it 5 ms late, the plugin can choose to start the DAW at exactly T0’ (which might be in ~0 ms or even slightly in the past). If slightly late, starting immediately is fine; if slightly early (due to clock skew), the plugin could wait until the precise moment. In practice, sub-10ms differences are negligible for transport timing, but we aim for sample-accuracy when possible.
Tempo (BPM) changes are handled similarly. If the DAW tempo is changed by the user, the plugin sends out a “BPM” message with the new value. All peers (and the GPU engine) will update their tempo setting. The GPU doesn’t actually need tempo for audio playback (it streams actual audio data), but tempo is needed for things like MIDI timing (scheduling beats) and for the UI (bars/beats display, or if remote peers drive arpeggiators, etc.). Likewise, if a remote user changes the session BPM (perhaps via a JAMNet control surface), a “BPM” command is broadcast. The plugin receives it and should apply it to the DAW. This is one of the tougher integrations – not all DAWs allow tempo changes coming from a plugin. For those that do (some may allow scripting or have a master clock that can be overridden), the plugin will call the host API to set the new tempo. For those that don’t, a fallback is to notify the user (e.g. flash “Tempo change: set DAW to 128 BPM”) or use MIDI tempo messages if supported. Ideally, future DAWs use protocols like Ableton Link, which do support external tempo sync; our plugin could act as a Link peer or similar. In any case, the API ensures the new BPM is known to JAMNet immediately – e.g. updating internal scheduling for predictive models, which might depend on tempo. We will expose a callback like onTempoChanged(double bpm, double confidence) so the host knows when a tempo change has been fully acknowledged by all sides (the “confidence” could indicate how certain it is that all peers are now in sync at that tempo).
Accurate Buffer Scheduling: To accommodate legacy pull/push models, the API will make the JAMNet data available in a way that suits the DAW’s scheduling. In a typical DAW, audio is pulled in buffers of fixed size each “block” (e.g. 128 samples every 2.9ms). MIDI is usually event-driven but delivered block-quantized as well. Our plugin must interface with this cycle. The shared buffer approach described earlier is crucial: the JAMNet engine can continuously produce audio in its internal block size (which might be very small, e.g. a few hundred samples or less, since it’s optimized for low latency) and write it into a ring. The DAW plugin, on its side, will pull from that ring whenever the DAW calls it to fill the next block. Because we have the GPU timebase mapping, the plugin knows exactly which portion of the JAMNet timeline the DAW is about to play. For example, suppose the DAW asks for audio block 100 (which corresponds to time 5.00–5.02 seconds in the song). The plugin converts that to the GPU clock domain (say 5.00s corresponds to GPU time T=5000000µs relative to session start) and looks in the buffer for audio with timestamps around that. If the network is running slightly ahead, the buffer will have that audio ready (with a timestamp slightly in the future); if the network is behind or just-in-time, the plugin might have to wait until it arrives. Fire-and-forget JSONL streaming means JAMNet is continuously sending audio irrespective of the DAW’s pulls. So a small jitter buffer in the plugin (perhaps 1–2 blocks worth, configurable) will absorb network variability. The Phase 4 API will provide controls to adjust this buffer size or target latency. If set to, say, 5ms, the plugin will always try to have 5ms of audio pre-fetched. This covers network jitter or scheduling jitter between CPU/GPU threads. It’s a tiny latency (much smaller than typical ASIO buffer sizes) and is acceptable in exchange for smooth playback.
For pushing data from DAW to JAMNet (e.g. live MIDI or audio from the DAW side to the session), the API again uses timestamped buffers. A MIDI event played on the DAW timeline at time X (say a note-on at bar 2 beat 3) will be timestamped by the plugin with the DAW’s time (which is synced to the GPU time via offset). The plugin sends it through JAMNet with that timestamp. When the remote peers receive it, their GPU-clock scheduling will place the note at the correct moment. Thanks to the clock sync, this can be accurate to sub-millisecond even over network. In effect, we achieve the illusion that the DAW’s sequencer and the JAMNet sequencer are one unified timeline.
Master Clock: GPU vs DAW vs Hybrid
GPU as Master Clock: The JAMNet architecture envisions the GPU as the “conductor” for the session. This means the source of truth for timing is the GPU’s high-res counter. In Phase 4, we strive to preserve this where possible. Running the GPU as master yields the tightest sync for distributed operation – all peers align to the GPU heartbeat, which is extremely stable and identical for all (assuming one authoritative GPU clock or synchronized GPUs). In this mode, the DAW essentially becomes a slave: it will subtly adjust its audio timing to stay locked to the GPU (via the methods described: minor buffer tweaks, using PNBTR to fill gaps). The advantage is maximal timing precision across the network and the DAW, and the metrics will show near-zero drift and minimal offset when things are working well. We recommend using GPU-master mode when the DAW is simply acting as another playback endpoint or recording inputs, and the creative focus is on the JAMNet session as a whole.
DAW as Master Clock: There are scenarios where the user might prefer the DAW to be the primary clock (e.g. when the session revolves around a particular studio setup, or the user trusts their audio interface’s word clock more). In this mode, the plugin/bridge will treat the DAW’s timeline as authoritative and command the JAMNet engine to adapt. How can JAMNet adapt? Since the GPU is running its own timeline, we would essentially feed it an adjusted tempo or resample factor to make its timeline match the DAW’s pace. For instance, if the DAW runs slightly faster, the plugin could send slight BPM nudges to JAMNet (not visible to user, but internally something like BPM 120.1 instead of 120) to cause the GPU scheduling to speed up. Alternatively, the JAMNet engine could simply accept the DAW’s clock offset info and skew its packet timing. The ClockDriftArbiter can detect if the DAW’s clock is leading or lagging and in master mode, instead of adjusting the DAW (since we promised to treat it as master), the arbiter adjusts the sending rate of audio packets to compensate. In practice, because the GPU is still needed for micro-timing, we might implement this by slaving the GPU clock to the DAW clock at a low frequency. E.g., every 50ms, compare DAW time vs GPU time; if DAW is ahead, insert a tiny wait in the GPU processing loop to slow down the GPU timeline by that fraction; if DAW is slow, skip an idle cycle to catch up. These adjustments are analogous to how a phase-locked loop would discipline an oscillator. Because the GPU timeline is software-controlled (via our compute shader scheduling), we can make it slightly elastic. In essence, the GPU still runs the show at microsecond scale, but its rate is continuously trimmed to match the DAW’s long-term rate.
Hybrid Synchronization: In many cases, a hybrid mode is ideal: neither side completely forces the other, but rather they converge to a common tempo/position. This is similar to Ableton Link’s approach, where each participant adjusts slightly so that everyone meets in the middle on tempo and phase. For JAMNet Phase 4, a hybrid mode would mean both the DAW and JAMNet adjust moderately. The GPU could handle high-frequency jitter (since it can react quickly) while the DAW makes slower coarse corrections. For example, on a short timescale (millisecond to 10ms), the DAW might not be able to react, so the GPU covers any needed adjustment via PNBTR and buffering; on a longer timescale (100ms+), if the DAW is consistently 1ms ahead every 100ms (drifting), the DAW plugin could slowly pull it back by 0.1% over the next second while the GPU slightly adjusts 0.1% on its side – meeting halfway. The result is a very stable sync without one clock racing against the other. We will implement this by using a dual-PLL concept: one PLL loop tries to keep DAW->GPU offset at zero by altering DAW’s pace (as a suggestion to the DAW or via sample slip), another loop tries to do the same by altering GPU pace. If the user has chosen a clear master (force mode), then one of these loops is set to a very stiff setting and the other to passive. If hybrid, both are soft. The parameters for these control loops (gain, response time) can be tuned; Phase 4 might expose them as advanced settings if needed.
Master Election: The API will incorporate the ability to elect or switch master clocks seamlessly. TOAST already has a notion of session master (it prints “Synchronizing” and has roles like CANDIDATE or MASTER in the ClockSyncPanel). We will build on that: for instance, if the DAW plugin has “Force Master” unchecked, it will participate in the election. Election could be as simple as: the peer with the lowest latency or a specific role becomes master, or a user-selected peer. We can use the sessionName or ID to impose an order (like lexically first is master unless overridden). In any case, the API should handle master handoff gracefully. If master changes (say user decides to make the DAW master mid-session), the system will perform a quick re-sync: the new master’s clock and timeline position is broadcast to all, and everyone re-bases to it (similar to a seek, but it’s a seek of time reference rather than song position). This might cause a tiny jump if clocks differed, but ideally by using the drift info we ensure there is little difference at the moment of switch.
In summary, we recommend GPU-master mode by default (to leverage the ultra-low latency design of JAMNet), but Phase 4 will support DAW-master and hybrid modes to maximize compatibility. This ensures the sync layer can integrate into various workflows: whether JAMNet is running as a background “band” that the DAW joins, or the DAW is the studio centerpiece and JAMNet clients are sideline collaborators.
DAW Integration Methods (API Delivery Mechanism)
To actually implement this sync API in real systems, we have a few options:
	•	Plugin SDK (Native Integration): Provide the synchronization functionality as a VST3/AU/AAX plugin (or a set of plugins) that the user can load into their DAW. The plugin would incorporate the JAMNet networking library (or at least the sync parts) and essentially act as a JAMNet client within the DAW. This has the advantage of tight coupling – the plugin can use the DAW’s API to control transport and tempo, and can inject audio/MIDI directly into the DAW’s signal chain. The Phase 4 spec leans heavily on this approach, since it allows using existing plugin standards. For example, a JAMNet VST3 plugin could have two components: one insert plugin for audio (to receive/send audio to JAM session as if it were an interface) and one MIDI clock device (to handle transport/tempo if needed). However, one plugin can also do both. Using a plugin also means shared memory is straightforward (the plugin and engine could be the same process if we link statically, or separate but with a memory map). We would ensure the plugin does minimal DSP itself – it mostly delegates to the GPU (e.g. by feeding data to the GPU for processing, if the GPU is accessible, or just receiving pre-processed streams from the network). This aligns with the “CPU Interface Layer: minimal CPU usage only for DAW integration” design goal.
	•	Shared Memory Bridge: In cases where the JAMNet engine runs as a separate process (for example, if using the provided Linux VM on Windows, or a standalone JAMNet app on Mac), a plugin might communicate via shared memory and synchronization primitives. The API would define a memory layout (like the ring buffers and message queue in shared RAM) that both the DAW plugin and the JAMNet engine access. This yields very low latency data transfer (no socket overhead, just memory copy) and can be as fast as function calls. A lightweight IPC (inter-process communication) mechanism like a named semaphore or spinlock can coordinate buffer access. The spec could specify, for instance: a named shared memory block JAMSyncBuffer_<sessionID> of fixed size that contains sub-buffers for audio, MIDI and command messages, along with read/write indices for each. The DAW plugin attaches to this by name when connecting to a session. Whenever the plugin writes a transport command in the shared command buffer, the JAMNet process will detect it (e.g. via semaphore or just polling every 1ms) and act, and vice versa. This method is very efficient and avoids overhead of serializing JSON over loopback network. It’s similar to how some low-latency audio drivers share memory between user-space and kernel.
	•	JSON-over-Socket: For maximum compatibility (especially if the DAW is on a different machine from the JAMNet engine, or for quick development), the sync API could also run over a standard socket (TCP or UDP). For example, the plugin might open a localhost TCP connection to the JAMNet application and send JSON messages (the same ones described in schema) for transport commands and clock sync. Audio could theoretically also go through this socket, but that would re-introduce overhead (unnecessary serialization of audio to JSON or similar). A hybrid approach might be: use a binary stream over TCP for audio, and JSON for control. If performance is acceptable (with efficient binary packing, it might be), this could simplify integration because it doesn’t require shared memory setup. It also has the advantage of working over a network, meaning a DAW on one PC could connect to a JAMNet session on another via this API – enabling scenarios like using a powerful GPU server for JAMNet and a separate DAW machine. The downside is slightly higher latency. However, since JAMNet’s JSONL core is already extremely optimized and uses UDP multicast, one idea is to simply treat the DAW as another TOAST multicast peer. The plugin could subscribe to the same multicast group (239.255.77.77:7777 by default) and send/receive TOAST frames like any peer. In fact, nothing stops us from implementing the DAW plugin as a TOAST client using the JAM Framework library. It could then receive all audio/MIDI frames natively. The main addition would be that it has to integrate those into the DAW timeline and feed out transport commands from the DAW. This is essentially using the network stack for integration – surprisingly, on localhost this can still be quite fast (microsecond-level) and it leverages existing code. So, the spec allows for a socket-based plugin that speaks TOAST. For performance-critical uses, though, we’ll likely go with either in-process (plugin contains engine) or shared memory if separate.
The API design itself is agnostic of these transport mechanisms – it defines the messages and data; implementations can choose the appropriate mechanism. For completeness, Phase 4 will likely offer a plugin that can operate in two modes: embedded (engine inside the plugin, good for single-machine use) and external (plugin connects to an already running JAMNet engine via IPC). This covers both scenarios.
Clock Re-basing on Transport Changes
As touched on earlier, handling seek (position jump) and tempo change requires resetting certain internal references:
	•	On Play Start: When going from stopped to play, we establish a new “zero” reference for the timeline. In TOASTer’s current implementation, pressing Play captures transportStartTime = now() (CPU high-res time) as reference. In a distributed context, the master will include its timestamp in the PLAY message. All followers do: transportStartTime = local_now - (remote_now_at_play - remote_start_timestamp). This effectively synchronizes their notion of “song time = 0 at play start” across machines. Our API will formalize this: when a PLAY is issued, the master timestamp in the message defines the epoch. If the DAW is slave, it might set an internal offset so that its bar/beat counter aligns. If DAW is master, the JAMNet engine will reset its own counters to match the DAW’s current sample time. This is done only on transitions from stop->play to avoid continuous drift.
	•	On Seek (Position): If a POSITION command is received (with a given song position P), the API will flush or realign buffers to the new point. For audio streaming, this could mean discarding any audio frames that correspond to the old timeline after P, and requesting fresh frames from position P onward. It effectively creates a discontinuity. The API should provide a callback to the host indicating a seek has occurred, so the DAW can, for example, clear its plugin delay buffers, reset any effects, etc., to avoid echoes from the old position. The clock conversion offset (GPU<->CPU) might not need to change drastically – since the underlying clocks continue – but the position counter (used to calculate beat/bar) resets to P. If the seek happens while playing, ideally all devices pause at a common safe point (like next bar) then jump. Our initial design will enforce an immediate pause: e.g. a POSITION command could implicitly include “stop” or be followed by a STOP. Then a new PLAY can resume from that spot. For user experience, this might appear as a quick glitch or pause when someone drags the timeline – which is acceptable given that collaborative timeline jumps are usually coordinated events.
	•	On Tempo Change: A BPM change is conceptually simpler – no timeline discontinuity, but the rate of progression changes. The main task is ensuring everyone uses the new tempo from the same moment. The command carries the new BPM and the timestamp of change. The DAW or master might change tempo gradually (some DAWs support tempo ramps). Our sync protocol for now assumes instantaneous change at a given barline or instant. If a BPM message is received mid-play, the plugin will apply it at the exact timestamp indicated (or as close as possible). If the DAW cannot change tempo smoothly, it might have to jump to the new tempo at the next measure boundary – in which case it would be wise to send the BPM command slightly ahead of time with a future timestamp (the API could allow scheduling: e.g. “at next bar, BPM = 140”). We can incorporate a bar alignment flag in the BPM message schema for this purpose. The JAMNet engine itself might not need any lead time; it can switch tempo for scheduling MIDI, but audio data is not tempo-dependent except in how we interpret timestamps for musical position. So as long as all agree on when the tempo changed, it’s fine.
In all cases of re-basing, latency compensation is recalculated immediately after. For example, right after a seek, the plugin should initiate a fresh sync handshake to account for any transient offsets introduced. The system could also momentarily increase the sync frequency to quickly settle after a major change. The result is that within a few sync intervals after a jump or tempo change, the clocks are back to steady lock.
Exposing Confidence and Delay Metrics
One of the valuable byproducts of the JAMNet system is rich performance metrics: latency measurements, drop-out rates, and PNBTR confidence scores. The Phase 4 API will surface these to the DAW/host to inform both users and potentially automated decisions in the host.
PNBTR Confidence: PNBTR (Predictive Neural Buffered Transient Recovery) is JAMNet’s neural “auto-dither” and packet loss concealer. It can predict audio or video frames ahead and fill in missing data. It produces a confidence score indicating how sure the model is about its prediction. In the current code, after generating a prediction, they log a message like “PNBTR: Audio prediction ready (97.5% confidence)” and store predictionConfidence in the framework state. We will expose this via the API. For example, after each audio buffer, the plugin could call jamSync.getLastPredictionConfidence() which returns a value 0.0–1.0. Alternatively, as part of the performance callback, a confidence percentage is provided. The DAW or plugin UI might use this to display a “network quality” indicator – e.g. a high confidence means even if packets are being lost, the audio is likely being reconstructed cleanly, whereas a low confidence (say < 50%) might warn that the audio could contain artifacts or gaps. A host could choose to record this metadata: for instance, if a remote musician’s guitar track had moments of low confidence due to network issues, the DAW could flag those regions in the recording for the user to review (maybe prompt to re-record or confirm). In real time, if confidence drops too much, the host might even automatically increase buffer size or request a higher redundancy setting (if adaptive).
Latency and Jitter Metrics: The API will provide the current measured latency between the DAW and the JAMNet engine (or between the DAW and session master). This could be end-to-end latency in microseconds. For example, if the DAW is playing back audio coming from a remote, latency would include network + buffering delay. The plugin can calculate an estimate by comparing the GPU timestamps on incoming audio to the arrival time. Also, the ClockArbiter directly computes network one-way latency (half the round-trip) for clock sync. In the TOASTer UI, they show “Latency: -- μs” which gets updated via a performance callback. We will route that same info to the DAW. A plugin UI could display “Network Latency: 300 µs” (or 0.3 ms). This is essentially the offset we’re compensating for. Jitter (variability) can be inferred from how much this latency measurement fluctuates. We might explicitly compute standard deviation over the last N sync intervals and expose that as well.
Delay Compensation for Host: DAWs typically allow plugins to report a processing delay (for plugin delay compensation, PDC). In this context, our plugin might introduce a small delay (due to network buffer). Ideally, we keep it under a few milliseconds, but it’s not zero. The API will let the plugin calculate its effective delay to the audio (for example, if we purposefully buffer 128 samples = ~3ms, plus network ~0.3ms, plus any alignment fudge). The plugin can then report this to the DAW’s PDC mechanism so that if you have local tracks playing alongside the network track, the DAW can align them. This ensures, for instance, that a metronome track in the DAW is perfectly in sync with the received JAMNet audio. Since our delay might adapt slightly (if network conditions change and we enlarge the buffer), we will update the reported delay accordingly. Some DAWs can’t change PDC on the fly, but we might at least log it or prompt. A more dynamic approach: because we know the exact timing of each packet, the plugin could schedule the audio for playback at the right time without needing global DAW PDC – essentially it does its own compensation internally. This is what we already plan by using timestamps. So the DAW timeline should remain aligned, but the explicit delay metric helps verify that.
Surfacing to Users: The sync API could feed these metrics to the DAW UI or plugin UI for user awareness. For example, a “JAMNet Status” window could show:
	•	Latency: 0.3 ms (with a color indicator green/yellow/red as in TOASTer).
	•	Drift: e.g. “-5 µs/sec” meaning the DAW is running slightly faster (negative drift means we’re slowing it down slightly).
	•	PNBTR Confidence: 98% (or separate audio vs video confidence if relevant).
	•	Active Peers: 3 peers connected.
	•	Throughput: e.g. 5.2 Mbps being received (informational). These mirror the performance stats already tracked in JAMNet. The plugin can simply subscribe to the same performanceCallback that TOASTer uses, to refresh these values in real time.
Using Metrics for Control: Beyond display, the host could use these metrics for decisions. For instance, if latency suddenly increases (maybe network congestion), the DAW could automatically increase its buffer size to prevent underruns. If PNBTR confidence drops, maybe disable certain effects or freeze tracks to reduce load. These are speculative, but Phase 4 provides the data; it’s up to host implementation to leverage it. At minimum, recordings made through the system could embed these metrics as metadata (so later one knows if a glitch at bar 32 was due to network issues).
Conclusion and Next Steps
In summary, the Phase 4 CPU-to-GPU sync API will enable seamless integration of legacy DAWs into the JAMNet GPU-accelerated environment. By establishing a robust clock sync mechanism (translating the GPU’s nanosecond frame counter to the DAW’s timeline), we ensure that audio and MIDI can flow between systems with microsecond accuracy. The API supports round-trip transport control – play, stop, seek, and tempo changes propagate in both directions so that all systems remain musically in sync. We’ve proposed a data schema and methods for implementing this, emphasizing minimal overhead and maximal use of existing infrastructures (JSONL messages and/or shared memory). The design allows either GPU or DAW to act as master clock, or a hybrid mode for mutual adaptability, using the ClockDriftArbiter logic to continuously minimize offset and drift. Finally, the sync layer will expose valuable confidence and delay metrics from the JAMNet core (like PNBTR’s predictive confidence and measured latency) to help the host system monitor and adjust to network conditions.
With this Phase 4 spec, JAMNet moves toward interoperability and DAW bridging, enabling musicians to incorporate the ultra-low-latency JAMNet streams into traditional production workflows. A DAW user will be able to treat a remote JAM session as if it were a local device – press play in Pro Tools and the band across the globe starts playing in time, or change the BPM in Ableton and the entire JAMNet session shifts tempo in perfect sync. All of this is achieved while leveraging the GPU-native performance of JAMNet under the hood, fulfilling the vision of a high-performance, cross-platform, distributed music production network. The next steps will be prototyping this plugin/bridge in a few host environments, refining the sync algorithms (especially the drift correction loop), and expanding support to many DAWs and plugin formats. Phase 4 will lay the groundwork for broad adoption of JAMNet by bridging the old (CPU DAWs) and the new (GPU real-time computing) in service of making real-time remote collaboration as tight as playing in the same room.
Sources: The design above is informed by the current MIDIp2p (JAMNet) codebase and documentation, including the definitions of TOAST and TOASTer, the existing transport sync implementation, and the clock sync and performance monitoring utilities in development. These sources illustrate the foundation on which Phase 4 builds, extending it to interface with external DAWs.
