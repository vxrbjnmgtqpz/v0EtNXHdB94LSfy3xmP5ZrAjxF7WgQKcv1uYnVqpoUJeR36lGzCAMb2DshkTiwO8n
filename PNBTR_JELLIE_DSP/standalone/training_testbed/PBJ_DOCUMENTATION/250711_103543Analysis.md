PNBTR+JELLIE DSP Training Testbed – Comprehensive Development GuideOverview: This guide outlines the architecture and development of the PNBTR+JELLIE training testbed – a GPU-accelerated audio processing engine with an ambitious sub-750µs latency target ￼. It combines proven video game engine patterns with real-time DSP techniques, Metal GPU compute shaders, JUCE framework integration, and a CMake build system ￼. The system uses a GPU-native, dual-AudioUnit Core Audio architecture, bypassing the usual JUCE audio loop to minimize latency ￼. We detail the core design principles, the end-to-end audio pipeline, the predictive PNBTR algorithm, debugging checkpoints, and the steps needed to go from prototype (point A) to a production-ready engine (point B) based on the latest research and implementation insights.Core Architectural PrinciplesTo achieve extreme low-latency and high performance, the software adopts patterns from modern game engines applied to audio DSP:	•	Frame Pacing & Real-Time Loops: The audio engine separates the fixed-rate audio callback from the variable-rate UI/graphics updates, similar to a game engine’s fixed update vs. render loop. The audio processing runs on a strict fixed timestep (e.g. 128 samples per callback, ~2.9ms at 48kHz) and must never miss its deadline, while the UI can drop frames if needed ￼ ￼. This ensures stable audio timing independent of GUI load.	•	Thread Separation: High-priority threads handle audio processing in isolation from the UI or other tasks, akin to Unity’s job system or Unreal’s separate render thread ￼ ￼. The audio callback thread runs at real-time priority and can dispatch heavy GPU processing work to background worker threads (or command buffers) so that the critical audio thread is never blocked ￼ ￼. This multi-threaded design guarantees the audio path remains glitch-free even as UI or other subsystems operate asynchronously.	•	Entity-Component System for DSP: The engine uses a modular, component-based design for audio processing nodes, inspired by game engine ECS patterns. Each DSP node can host multiple processing components (filters, analyzers, etc.) that can be added or removed dynamically ￼ ￼. A central DSP graph manages these nodes and their connections, processing them in order of dependencies ￼. This yields a flexible pipeline where new audio effects or neural network modules (like JELLIE or PNBTR components) can be hot-swapped without monolithic code changes.These principles lay the groundwork for a responsive, maintainable audio engine that can leverage GPU acceleration and handle real-time constraints.Metal GPU Audio Processing PipelineDual AudioUnit Design: The audio engine is implemented with two Core Audio AudioUnits – one for input capture and one for output playback – bridged by a custom MetalBridge that runs GPU compute shaders for processing ￼. Audio flows directly from the input AudioUnit’s callback into the Metal GPU pipeline, then back to the output AudioUnit’s callback, creating a zero-copy, low-latency path from input device to output device ￼. By avoiding any intermediate buffers or the default JUCE processing callback, latency is kept to an absolute minimum.Seven-Stage GPU Pipeline: Once audio enters the MetalBridge, it passes through a series of seven processing stages, all executed on the GPU as compute shader kernels each audio frame ￼ ￼:	1.	Input Capture – Captures incoming audio frames (from microphone or interface) and applies input gain. Handles record-arming logic for enabling/disabling input capture ￼.	2.	Input Gating – Applies noise suppression and gating, letting audio through only when a valid signal is present (e.g. voice activity detection to cut silence) ￼.	3.	Spectral Analysis – Performs an FFT-based analysis in real time (using a GPU shader). Currently a simplified custom FFT is used, but this can be upgraded to Metal Performance Shaders for higher performance ￼ ￼. The spectral data is used for visualization and possibly for guiding the neural processing.	4.	JELLIE Preprocessing – Prepares the audio for neural network processing. This stage likely normalizes audio, chunks it, and attaches timing metadata. For example, it packages audio samples into a custom JELLIESample structure with timestamps and channel info ￼ ￼. This ensures the subsequent PNBTR stage has the context needed for prediction.	5.	Network Simulation – Simulates network conditions like packet loss and jitter. In the training testbed, this stage can intentionally drop or delay samples to test the system’s ability to recover lost audio ￼. It marks missing samples and generates a loss pattern (e.g. an array of flags) to feed into the reconstruction stage ￼. This is crucial for developing the PNBTR algorithm under realistic conditions.	6.	PNBTR Reconstruction – Predictive Neural Bit-Transparent Reconstruction is the core innovation of the system. In this stage, the GPU uses a neural-inspired algorithm to predict and fill in lost audio data, effectively reconstructing missing waveform segments in real-time ￼. PNBTR treats the discrete audio stream as an analog continuous signal, using prior samples and learned models to guess the continuity when data is lost or delayed ￼ ￼. It operates entirely on the GPU buffers (no round-trip to CPU), dispatching a Metal compute kernel that takes the prepared audio and loss pattern as input and outputs a fully reconstructed audio buffer ￼ ￼. The PNBTR algorithm has tunable parameters – e.g. neural confidence threshold, lookback window, smoothing factor – which can be adjusted for different content (speech vs music vs sustained tones) ￼ ￼. Those parameters can even adapt on the fly based on measured network quality metrics (packet loss, SNR, etc.) to balance aggressiveness of prediction vs. audio fidelity ￼ ￼.	7.	Visual Feedback – Renders real-time visualizations (waveforms, spectra, indicators) of the audio and the system’s state. A final shader might draw a spectrum or waveform texture that the GUI can display. It also shows status like a JELLIE armed (recording) indicator in red and PNBTR active indicator in blue by glowing the visuals when those are engaged ￼ ￼. This stage runs in parallel with audio processing, writing to a texture or buffer for the UI without disturbing the audio thread.All seven stages are executed in sequence each audio frame by issuing GPU commands via a single Metal command buffer ￼ ￼. This command buffer is committed and the thread waits for its completion at a synchronization point to ensure the GPU work is done before audio output needs the data ￼. Thanks to the GPU’s parallelism, the entire pipeline can process a buffer of audio in under 750µs, meeting the project’s real-time goal ￼.PNBTR: Predictive Neural Audio ReconstructionPNBTR (Predictive Neural Bit-Transparent Reconstruction) is the algorithmic heart of the system, enabling what would otherwise be impossible ultra-low latency audio transport. PNBTR’s goal is to eliminate network latency as a meaningful factor by intelligently predicting missing audio samples in real time ￼. In practice, it means if audio packets are lost or arrive late (such as in an online jamming session), the system fills the gaps so that the listener never experiences dropouts. PNBTR models the audio stream as if it were a continuous analog signal and uses recent history to guess the next samples seamlessly ￼ ￼.Under the hood, PNBTR is implemented as one or more Metal compute shaders (e.g. PNBTRReconstructionShader.metal) that run every audio frame on the GPU. These shaders take in the audio data with gaps (from the Network Simulation stage) and output a fully reconstructed audio buffer. For example, the shader has access to a struct of PNBTR parameters (PNBTRUniforms) and the array of network audio samples with markers for lost frames ￼ ￼. It then produces new samples for any gaps, possibly using predictive filters or neural network logic encoded in the shader. The term “bit-transparent” indicates the reconstruction strives to be indistinguishable from the original signal at the bit level under good conditions.Key aspects of PNBTR include:	•	Tunable Behavior: PNBTR can be tuned for different scenarios by adjusting its parameters. For instance, for natural speech one might choose a conservative setting (e.g. threshold 0.8, short lookback) whereas for music or sustained tones a more aggressive prediction (threshold >1.0, longer lookback, more smoothing) can be used ￼ ￼. These settings control how bold the prediction is versus how much it smooths over gaps.	•	Real-Time Adaptation: The system can monitor network metrics like packet loss rate or signal-to-noise ratio and adjust PNBTR parameters on the fly ￼ ￼. For example, with high packet loss, it can increase the aggressiveness of prediction (lowering quality threshold, using more history), whereas with stable network it can dial back to favor absolute fidelity ￼ ￼. This ensures optimal audio quality under varying conditions.	•	GPU-Only Processing: All PNBTR computation happens on the GPU without CPU intervention. The engine prepares GPU buffers for parameters and audio data, then dispatches a compute kernel across the needed number of threads (frames) to perform the reconstruction ￼ ￼. The output is written to a GPU buffer that becomes the audio output for that frame. This design minimizes CPU load – the CPU simply kicks off GPU work and then uses the result – and thereby keeps up throughput even for large numbers of audio channels or effects.In sum, PNBTR is what makes the “magic” of gapless audio possible. It forecasts audio to bridge network hiccups, allowing the system to maintain continuity and ultra-low latency where traditional systems would glitch. Combined with JELLIE (which handles preprocessing and presumably any machine-learning-based enhancement of the audio before prediction), PNBTR transforms the engine from a reactive system to a predictive, proactive audio engine ￼.End-to-End Signal Flow DebuggingBecause of the complexity of the pipeline, thorough signal flow validation is critical. The project implements a systematic six-point checkpoint system to trace the audio path from input to output and catch any stage that fails or introduces silence ￼. These debug checkpoints are inserted at strategic locations:	•	Checkpoint 1: Hardware Input – Right at the input AudioUnit callback, verify that audio frames are coming in from the hardware. The code measures the max amplitude of the input buffer and logs a warning if it’s completely silent (which could indicate a microphone issue or permissions problem) ￼ ￼. This ensures the engine is actually receiving data from the source.	•	Checkpoint 2: CoreAudio ➜ MetalBridge Transfer – In the MetalBridge::processAudioBlock function (where the input callback hands off to the GPU pipeline), confirm that the data received is not all zeros. For example, the code computes the peak of the input sample block and logs “[❌ CHECKPOINT 2] SILENT METALBRIDGE INPUT” if nothing non-zero was passed along ￼ ￼. This catches any issue in handing off audio from the input unit to the GPU (e.g., if the AudioUnit wasn’t configured properly or data format mismatch causing silence).	•	Checkpoint 3: GPU Buffer Upload – After copying the audio samples into the GPU memory (e.g., into an MTLBuffer for the audio ring buffer), the code checks the contents of that buffer. It duplicates the mono input into stereo channels in the GPU buffer and then finds the peak value in the GPU buffer to ensure the upload succeeded ￼ ￼. If the GPU buffer shows zero level, it logs an error “[❌ CHECKPOINT 3] SILENT GPU BUFFER – Upload failed” ￼. This verifies that the data was correctly transferred to GPU memory before processing.	•	Checkpoint 4: GPU Processing Output – Right after executing all the GPU shader stages (stages 1–7) and before handing audio to the output, the system checks the resulting processed buffer. The command buffer is committed and waited on, so by this point the reconstructedBuffer (output of PNBTR stage) contains the final audio for output ￼ ￼. The code scans through this final buffer for a peak value and logs “[❌ CHECKPOINT 4] SILENT GPU OUTPUT – Shader processing failed” if the output is all zeroes ￼. This catches any internal failure in the GPU processing pipeline (for instance, if a shader didn’t execute or produced silence due to a bug).	•	Checkpoint 5: GPU ➜ Output Buffer Transfer – After the GPU has processed the audio, the results are downloaded or copied into the output AudioUnit’s buffer (the CPU-accessible buffer that will be given to the hardware output). Checkpoint 5 verifies this transfer. It logs the peak of the downloaded audio and flags “[❌ CHECKPOINT 5] SILENT DOWNLOAD – GPU→CPU transfer failed” if something went wrong at this stage ￼. This ensures the data made it out of GPU memory and into the output buffer that Core Audio will send to the speakers. Issues here could arise from not synchronizing the GPU results before using them on CPU, etc. ￼.	•	Checkpoint 6: Hardware Output – Finally, in the output AudioUnit’s render callback (feeding the speaker/output device), the system confirms that the audio delivered to the hardware is non-silent. It logs “[❌ CHECKPOINT 6] SILENT OUTPUT – Final stage failed” if the output buffer has no signal ￼. This would catch problems like the ring buffer between GPU thread and output thread being empty at the wrong time or an AudioUnit output misconfiguration ￼. Essentially, it double-checks that audio actually made it to the DAC.Each checkpoint provides a clear log message with a ✅ or ❌ indicator and hints at what might be wrong, greatly simplifying troubleshooting. For example, if a Checkpoint 2 failure is seen (silent MetalBridge input), one would focus on the AudioUnit format negotiation and data handoff. If a Checkpoint 4 failure occurs, one debugs the Metal shader logic. This systematic approach ensures that every stage from microphone input to speaker output is verified, and it aligns with the six-point validation checklist the team planned ￼ ￼.Key Implementation Challenges & SolutionsDuring development, several critical issues were identified. The following are the main challenges (point A) and how the design or upcoming fixes address them to reach a robust solution (point B):	•	AudioUnit Stream Format Mismatch (Error -50): A -50 (paramErr) from AudioUnit indicates a stream format problem ￼ – often the AudioStreamBasicDescription (ASBD) for input/output doesn’t match what the hardware or AudioUnit expects. In our case, the input and output AudioUnits must be configured with the same sample rate, channels, and data format as the hardware. The solution is to explicitly set the ASBD and then retrieve it back with AudioUnitGetProperty to verify it was accepted as expected ￼. Logging the formats after setting them helps detect any negotiation failure (e.g., if the device is 48kHz but the AudioUnit is still at 44.1kHz). We also ensure the AudioBufferList structure in callbacks is correctly set (e.g., mNumberBuffers matches channel count, mDataByteSize matches buffer size) to avoid misconfiguration. By fixing the stream format negotiation and buffer list setup, the AudioUnit -50 errors should be resolved and audio can flow.	•	GPU Transfer Latency on Audio Thread: Initially, there were concerns about the audio thread stalling during GPU operations – for example, waiting for GPU transfers (upload/download) to complete could block the real-time audio callback ￼. This can cause dropouts if not handled. The solution is to use asynchronous GPU command buffers and double-buffering so that the audio thread never waits in place. The Metal command buffer submission for a frame should be non-blocking; the audio thread can push data to the GPU and immediately return, with the GPU doing work in parallel. Any read-back (download) from GPU can be synchronized carefully or, if possible, done via double buffers so the next audio frame isn’t stuck waiting. Essentially, eliminate any polling or waitUntilCompleted on the audio callback thread. Profiling with Xcode Instruments (Metal profiler) is used to ensure GPU operations overlap with CPU and do not introduce undue latency ￼.	•	Real-Time Safety (Memory and Threading): Real-time audio threads must avoid any operations that could block or unpredictably delay. A code audit revealed potential dynamic memory allocations or locks happening in the audio processing path, which is dangerous in a real-time context ￼. To move from prototype to production (point B), we must remove all heap allocations and locks from the audio callback. This means pre-allocating buffers (audio buffers, MTLBuffers, etc.) ahead of time, reusing them each frame, and using lock-free data structures or atomic flags for communication. The current design already factors this in (e.g., ring buffers for audio are allocated upfront), but the implementation needs a thorough review to ensure things like new or malloc are not called in the callback. By pre-allocating all resources and using lock-free synchronization, we adhere to real-time safe guidelines, preventing glitches.	•	Signal Chain Validation Gaps: Earlier versions lacked sufficient logging to pinpoint where audio data was getting lost. The introduction of the six debug checkpoints (described above) fills this gap ￼. Developers should make sure these checkpoints remain in the code (perhaps compiled in a debug mode) until the system is stable. Each stage’s verification (input signal present, data in GPU buffer, etc.) provides immediate insight into any malfunction. By systematically following these checkpoints during testing, one can isolate and fix issues stage by stage rather than guessing blindly. This transforms the debugging process into a stepwise validation, significantly speeding up the move from a faulty A state to a fully working B state.	•	Audio I/O Buffer Configuration: The project must carefully manage audio buffer sizes and formats. For instance, if the hardware IO buffer size is 128 frames, all pipeline stages and buffers need to accommodate that in a consistent way. A mismatch here can cause errors or dropouts. The solution is to retrieve the device’s preferred buffer size and configure the AudioUnit buffers and internal ring buffers to match. The included test scripts and validation routines (e.g., phase_a_validation.sh, etc., observed in the repository) likely automate testing various buffer sizes. This ensures the pipeline handles different buffer configurations reliably and helps avoid issues like underruns or overruns in the ring buffer.By addressing each of these challenges with targeted solutions, the project moves from a partially working prototype with glitches to a stable, production-ready audio engine. Each fix brings the implementation closer to the theoretical performance the architecture promises.Development Workflow and ToolsBuilding a complex audio-GPU system requires a robust development workflow:	•	CMake Build System: The project uses CMake to manage the build, including compiling Metal shader sources and integrating JUCE (for the GUI and audio device handling). For example, the CMakeLists defines the project and ensures the C++17 standard is used (a “lesson learned” to set the standard early) ￼. It leverages juce_add_gui_app to create the standalone app “PNBTR+JELLIE Training Testbed” with JUCE, and includes custom build steps for Metal shader files so they are compiled into the binary. This cross-platform build setup makes it easier to maintain the project and eventually migrate to other plugin formats.	•	Testing and Validation Scripts: A series of test scripts (e.g., phase_a_validation.sh, phase_b_summary.sh, etc., seen in the repository) and automated tests accompany the development. These likely run through various scenarios: audio loopback tests, network simulation scenarios, etc., to verify each subsystem. The testing protocol is systematic – following the validation checklist (checkpoints 1–6) and measuring performance metrics at each phase ￼. By automating these tests, developers can quickly catch regressions and ensure each change meets real-time requirements.	•	Debugging & Profiling Tools: The team uses Instruments.app (Xcode Instruments) with the Metal frame debugger and time profiler to analyze GPU performance and audio thread timing ￼. This helps confirm that GPU compute work stays within the allotted time (<750µs) and that CPU threads are not starved. Additionally, verbose logging (as seen with the checkpoint logs and error tables in the documentation) provides insight into AudioUnit errors and performance stats in real time. The documentation even includes comprehensive error code tables for Core Audio, so that any numeric error (like -10851) can be looked up immediately with its meaning and remedy ￼.	•	Future Integration (AUv3 plugin): While currently a standalone app for testing, the development plan includes migrating to an Audio Unit v3 plugin for DAW integration ￼. The architecture is designed to separate the core DSP logic from the UI, which will facilitate wrapping the DSP engine in a plugin container. The code is organized so that the core processing (e.g. a PNBTRProcessor class) can run headless, and a lightweight AUv3 view controller can host it in a plugin context ￼ ￼. This forward-looking approach means the work on the training testbed directly translates to a production plugin down the line.By maintaining a rigorous development pipeline with these tools and practices, the project ensures that moving from A (development prototype) to B (final product) is a controlled, efficient process. Each improvement is tested and verified, and the team has confidence in reproducibility of results across environments.ConclusionThe PNBTR+JELLIE DSP system represents a revolutionary audio engine architecture that marries low-latency game engine techniques with the power of GPU computing for audio. The core architecture has been validated to achieve the targeted performance – a sub-millisecond end-to-end latency, GPU-accelerated processing for all stages, and intelligent predictive gap filling for network audio ￼ ￼. The remaining work chiefly involves refining the implementation details: fixing AudioUnit format mismatches, eliminating any non-real-time-safe operations, and thoroughly testing each stage with the provided debugging checkpoints. The good news is that the architecture is fundamentally sound – the design principles and pipeline have proven effective ￼. By systematically addressing the known issues (as outlined above) and following the comprehensive validation checklist, the project can confidently progress from its current state (A) to a robust, production-ready state (B). The result will be a pioneering audio transport engine (code-named “Peanut Butter & JELLIE”) that achieves what was once thought impossible: glitch-free, predictive real-time audio streaming with imperceptible latency. ￼ ￼
